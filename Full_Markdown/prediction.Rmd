---
title: "Predicting The Election"
author: "Samuel Thau"
date: "11/1/2020"
output:
  rmarkdown::html_document:
    theme: cerulean
    
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "../Posts/") })    

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(ggplot2)
library(usmap)
library(janitor)
library(usmap)
library(data.table)
library(geofacet) ## map-shaped grid of ggplots
library(statebins)
library(reshape2)
library(apcluster)
library(MASS)
library(stabledist)

# read in all data
nat_popvote_df <- read_csv("../Data/popvote_1948-2016.csv")
nat_poll_df    <- read_csv("../Data/pollavg_1968-2016.csv")

state_popvote_df <- read_csv("../Data/popvote_bystate_1948-2016.csv") %>%
  mutate(D_pv = D/total, R_pv = R/total, vote_margin = abs(D_pv - R_pv))

state_poll_df <- read_csv("../Data/pollavg_bystate_1968-2016.csv") %>% 
  filter(days_left <= 21) %>%
  group_by(year, state, candidate_name, party) %>%
  dplyr::summarize(avg_poll = mean(avg_poll)/100) %>% 
  dplyr::select(year, state, party, avg_poll)

state_poll_df$state <- c(state.abb, 'DC')[match(state_poll_df$state, c(state.name, 'District of Columbia'))]
state_popvote_df$state <- c(state.abb, 'DC')[match(state_popvote_df$state, c(state.name, 'District of Columbia'))]

nat_econ <- read_csv("../Data/econ.csv")
state_rdi <- read_csv("../Data/state_rdi.csv")
vep_df <- read_csv("../Data/vep_1980-2016.csv")
vep_df$state <- c(state.abb, 'DC')[match(vep_df$state, c(state.name, 'District of Columbia'))]


demo <- read_csv("../Data/demographic_1990-2018.csv") %>% dplyr::select(-total)

cost <- read.delim("../Data/covi_1996-2016.txt", header = TRUE, sep = "|", stringsAsFactors = FALSE)
colnames(cost) <- c("state", "1996", "2000", "2004", "2008", "2012", "2016")
cost_df <- melt(cost, 
                id.vars = c("state"), 
                value.name = "year")
colnames(cost_df) <- c("state", "year", "cost")
dc_cost <- cbind(rep('DC', 6), seq(from = 1996, to = 2016, by = 4), rep(0,6))
colnames(dc_cost) <- c("state", "year", "cost")
cost_df <- rbind(cost_df, dc_cost)
cost_df$year <- as.numeric(as.character(cost_df$year))

wide_polls <- state_poll_df %>% filter(party == "republican") %>% 
  left_join(state_poll_df %>% filter(party == "democrat"), by = c("state", "year"))

colnames(wide_polls) <- c("r_name","year", "state", "rep", "rep_poll","d_name", "dem", "dem_poll")

wide_polls <- wide_polls %>% mutate(poll_margin = rep_poll - dem_poll) %>%
  dplyr::select("state", "year", "poll_margin")

turnout_votes <- state_popvote_df %>% dplyr::select("state", "year", "total", "vote_margin") %>%
  left_join(wide_polls %>% dplyr::select("state", "year", "poll_margin"))

turnout_df <- turnout_votes %>%
  left_join(demo, by = c("state" = "state", "year" = "year")) %>%
  left_join(cost_df, by = c("state" = "state", "year" = "year")) %>%
  left_join(vep_df, by = c("state" = "state", "year" = "year"))

## Load 2020 polling data
poll_2020_df <- read_csv("../Data/presidential_poll_averages_2020.csv")

elxnday_2020 <- as.Date("11/3/2020", "%m/%d/%Y")
dnc_2020 <- as.Date("8/20/2020", "%m/%d/%Y")
rnc_2020 <- as.Date("8/27/2020", "%m/%d/%Y")

colnames(poll_2020_df) <- c("year","state","poll_date","candidate_name","avg_support","avg_support_adj")

poll_2020_df <- poll_2020_df %>%
  mutate(party = case_when(candidate_name == "Donald Trump" ~ "republican",
                           candidate_name == "Joseph R. Biden Jr." ~ "democrat"),
         poll_date = as.Date(poll_date, "%m/%d/%Y")) %>%
  mutate(days_left = round(difftime(elxnday_2020, poll_date, unit="days")),
         weeks_left = round(difftime(elxnday_2020, poll_date, unit="weeks")),
         before_convention = case_when(poll_date < dnc_2020 & party == "democrat" ~ TRUE,
                                       poll_date < rnc_2020 & party == "republican" ~ TRUE,
                                       TRUE ~ FALSE)) %>% filter(!is.na(party)) 

poll_2020_clean <- poll_2020_df %>% filter(days_left <= 21) %>% 
  group_by(state, candidate_name, party) %>%
  summarize(avg_poll = mean(avg_support_adj)/100) %>% filter(state != "National") %>% 
  dplyr::select(state, candidate_name, avg_poll, party) %>%
  filter(state != "NE-1", state != "NE-2", state != "ME-1", state != "ME-2")

wide_poll_2020 <- poll_2020_clean %>% filter(party == "democrat") %>% 
  left_join(poll_2020_clean %>% filter(party == "republican"), 
            by = c("state" = "state"))
colnames(wide_poll_2020) <- c("state", "biden", "dem_poll", "dem", "trump", "rep_poll", "rep")  
wide_poll_2020$state <- c(state.abb, 'DC')[match(wide_poll_2020$state, c(state.name, 'District of Columbia'))]
poll_margin_2020 <- wide_poll_2020 %>% mutate(poll_margin = abs(rep_poll - dem_poll)) %>% 
  dplyr::select(state, poll_margin)

turnout_2020 <- poll_margin_2020 %>% 
  left_join(demo %>% filter(year == 2018), by = c("state"= "state")) %>% 
  dplyr::select(poll_margin, Asian, Black, Hispanic, Indigenous,
         White, Female, Male, age20, age3045, age4565, age65) %>% 
  add_column(vote_margin = NA) %>% add_column(cost = NA) %>% add_column(total = NA) %>% 
  add_column(VEP = NA) %>% add_column(VAP = NA)
turnout_2020$year <- rep(2020, 51)


## scale predictors
num_pred <- c("vote_margin", "Asian", "Black", "Hispanic", "Indigenous", "White", 
              "Female", "Male", "age20", "age3045", "age4565", "age65",
              "last_vote_margin", "last_turnout")

turnout_scaled <- rbind(turnout_df, turnout_2020)
turnout_scaled <- turnout_scaled %>% filter(year > 1984) %>% 
  group_by(state) %>%
  mutate(last_vote_margin = dplyr::lag(vote_margin, k = 1, default = NA)) %>%
  mutate(last_turnout = dplyr::lag(total, k = 1, default = NA)) %>%
  ungroup() %>%
  dplyr::select(state,VEP,total, year, poll_margin, num_pred) %>% group_by(year) %>% mutate_at(num_pred, scale) %>% 
  ungroup() %>% mutate_at(vars(VEP), funs(round(.))) %>% ungroup() %>% filter(year > 1990)


##turnout model
turnout_model <- glm(total ~ last_vote_margin + poll_margin +Black + Hispanic + 
                       Asian + White + Male + age20 + age3045 + age4565 +
                       last_turnout + state, 
                     data = turnout_scaled %>% filter(year < 2020), family = "poisson")

## do predictions
turnout_pred_df <- turnout_scaled %>% filter(year == 2020)
turnout_2020_pred <- as.data.frame(predict(turnout_model, turnout_pred_df, type = "response"))
turnout_2020_pred$state <- turnout_pred_df$state
colnames(turnout_2020_pred) <- c("turnout", "state")


```

```{css, echo=FALSE}
.scroll-100 {
  max-height: 250px;
  overflow-y: auto;
  background-color: inherit;
}
```

## Introduction
At the time of writing, we are two days away from the 2020 presidential election between Donald Trump and Joe Biden. Many believe that this may be the most important election in the history of the United States. The question on everyone's mind is the same: who is going to win? I'll discuss the model I have built, and then show my prediction. There are four parts to the model:

1. Estimating turnout in each state. 

2. Estimating vote share for each candidate in each state. 

3. Estimating national polling error. 

4. Simulating the election based on the estimated parameters. 

I'll go through each section in turn, examine the results, and then decide what I think of this model. 

## The Basic Structure
The end goal of this model is to estimate the number of voters that for each candidate in each state. Given that I want to do this with a probibalistic model, the natrual choice to do that is with draws from binomial random variables:

$$\textrm{Votes}_{ic}\sim\textrm{Bin}(n_{i}, p_{ic})$$
The subscripts $i$ denotes each state, and the subscript $c$ denotes each candidate. For both Trump and Biden, each simulation of the election is a draw from a set of 102 random variables: two binomial distributions for each state, with 51 different turnout values and 102 different probability values.Simulating these draws is exactly step four in the process that I explained in the introduction. In my model, both $n_i$ and $p_{ic}$ are also random variables, making this a sort of hierarchical model. Let's being with looking at how turnout is estimated. 

## Estimating Turnout
I estimate turnout using a pooled model, across all states and across every election since 1992 After spending some time looking at the data, I settled on using a poisson regression to estimate turnout. Poisson models are a form of generalized linear models, based on the [poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution). Because the data is a set of discrete counts, this seemed like a reasonable choice to me[^1]. I used a great number of covariates to estimate the turnout model. This includes demographic data, the polling margin in the state[^2], lagged variables for the previous election's turnout and voting margin, and a state fixed effect indicator. For a full desription of the data, see [Appendix: Data] at the end of this post. We can look at the full output from the model:

```{r, echo=FALSE,  warning = FALSE, message = FALSE, class.output="scroll-100" }
 summary(turnout_model)
```

The key numbers for in sample fit are the null and residual deviances, at the very bottom of the funciton call. The residual deviance being significantly smaller, `r format(turnout_model$null.deviance, scientific = FALSE)` versus `r format(turnout_model$deviance, scientific = FALSE)`, suggests that there is significant improvement against the null[^3] model. Becuase of scaling discussed in the [Appendix: Data] section, the interpretation of coefficients is not particularly enlightening. One thing to remember is that becuase this is a poisson regression, the signs of the coefficients indicate which direction the log of the total turnout would move. The coefficient on the poll margin is positive, which actually goes against some research - it suggests that a greater poll margin leads to a higher turnout. It could be that higher poll margins are the result of more people deciding to vote for a particular party, meaning that turnout is influencing the margin, suggesting that the regression may not properly capture cause and effect.

We can also look at out of sample validation. In this case, I conducted leave one out validition by year, and then calculated the sum of the squares of the residuals in each state as a measure of fit.

```{r echo = FALSE, return = FALSE, warning = FALSE, message = FALSE}
validate_turnout <- function(yr){
  temp_df <- na.omit(turnout_scaled %>% filter(year != yr, year < 2020))
  
  if(yr != 2016){
    pred_df <-na.omit(turnout_scaled %>% filter(year == yr))
  }else{
    pred_df <- na.omit(turnout_scaled %>% filter(year == yr, state != "DC"))
  }
  
  temp_model <- glm(total ~ last_vote_margin + poll_margin +Black + Hispanic + 
                       Asian + White + Male + age20 + age3045 + age4565 +
                       last_turnout + state, 
                     data = temp_df, family = "poisson")
  predicted_turnout <- predict(temp_model, pred_df, type = "response")
 
  err <- sum((predicted_turnout - pred_df$total)^2)
  err
}

year_list <- seq(from = 1992, to = 2016, by = 4)
turnout_error <- sapply(year_list, validate_turnout)

```

```{r, echo = FALSE, return = FALSE, warning = FALSE, message = FALSE}
library(kableExtra)
out_error <- cbind(turnout_error, year_list)
colnames(out_error) <- c("Sum of Squares of Residuals", "Year Preidcted")

kbl(out_error)  %>% kable_paper(full_width = F) %>% kable_styling(position = "center")

```

While these numbers do seem quite high, it is important to remember that these values are squared. Numbers that are on the order of ten to the twelfth represent being off by millions of votes, which in an election of roughly 120 million votes is quite good The final thing to do with this model is predict the turnout for each state in 2020, which we will use later when we simulate the election.

## Estimating Vote Share
```{r echo = FALSE, return = FALSE, warning = FALSE, message = FALSE}
econ_df <- state_rdi[,1:2]
colnames(econ_df) <- c("state","rdi_q2")
econ_df$rdi_q2 <- as.numeric(econ_df$rdi_q2)

for(i in seq(from = 18, to = 290, by = 16)){
  temp <- state_rdi[,c(1,i)]
  colnames(temp) <- c("state","rdi_q2")
  temp$rdi_q2 <- as.numeric(temp$rdi_q2)
  econ_df <- rbind(econ_df, temp)
}

#add years and national gdp growth
year_list <- seq(from = 1948, to = 2020, by=4)
year_append <- rep(year_list[1],51)
nat_econ_sub <- nat_econ %>% filter(year %in% year_list, quarter == 2) %>% 
  dplyr::select(year, GDP_growth_qt)
gdp_append <- as.numeric(rep(nat_econ_sub[1,2],51))

for(i in seq(from = 2, to=length(year_list), by =1)){
  new_year = rep(year_list[i], 51)
  new_gdp = as.numeric(rep(nat_econ_sub[i,2], 51))
  year_append = append(year_append, new_year)
  gdp_append = append(gdp_append, new_gdp)
}

#append year and national gdp to the econ df
econ_df$year <- year_append
econ_df$gdp <- gdp_append
econ_df$state <- c(state.abb, 'DC')[match(econ_df$state, c(state.name, 'District of Columbia'))]


## Merge datasets and clean
poll_data <- nat_popvote_df %>% dplyr::select("year","party","winner", "incumbent","incumbent_party")


reg_df <- state_poll_df %>% left_join(econ_df, by=c("state"="state","year"="year")) %>% 
  left_join(state_popvote_df, by= c("state"="state", "year"="year")) %>%
  left_join(poll_data, 
            by  = c("year"="year","party" = "party" ))


reg_df <- reg_df %>% 
  mutate(inc_party = case_when((party=="democrat" & incumbent_party==TRUE) ~ "democrat", 
                               (party=="democrat" & incumbent_party==FALSE) ~ "republican",
                               (party=="republican" & incumbent_party==TRUE) ~ "republican",
                               (party=="republican" & incumbent_party==FALSE) ~ "democrat")) %>%
  mutate(chl_party = case_when((party=="democrat" & incumbent_party==FALSE) ~ "democrat", 
                               (party=="democrat" & incumbent_party==TRUE) ~ "republican",
                               (party=="republican" & incumbent_party==FALSE) ~ "republican",
                               (party=="republican" & incumbent_party==TRUE) ~ "democrat"))

reg_df <- reg_df %>% 
  mutate(inc_pv = case_when((inc_party=="republican") ~ R_pv2p, 
                            (inc_party=="democrat") ~ D_pv2p)) %>% 
  mutate(chl_pv = case_when(chl_party == "democrat" ~ D_pv2p, 
                            chl_party == "republican" ~ R_pv2p))

reg_df <- reg_df %>% mutate(incumbent_win = case_when(inc_party == "republican" & R_pv2p > D_pv2p ~ 1,
                                                      inc_party == "republican" & R_pv2p < D_pv2p ~ 0, 
                                                      inc_party == "democrat" & R_pv2p < D_pv2p ~ 1,
                                                      inc_party == "democrat" & R_pv2p > D_pv2p ~ 0))

reg_df <- reg_df %>% 
  mutate(inc_votes = case_when((inc_party=="republican") ~ R, 
                               (inc_party=="democrat") ~ D)) %>% 
  mutate(chl_votes = case_when(chl_party == "democrat" ~ D, 
                               chl_party == "republican" ~ R))

reg_df <- reg_df %>% left_join(vep_df, by = c("state" = "state", "year" = "year")) 
reg_df <- reg_df %>% left_join(demo, by = c("state" = "state", "year" = "year"))
reg_df <- na.omit(reg_df)
reg_df <- reg_df %>% mutate_at(vars(VEP), funs(round(.)))


gdp_2020 <- rep(-0.0949, 102)
poll_2020_clean$gdp <- gdp_2020
pred_df <- poll_2020_clean %>% left_join(state_rdi[,c(1,290)], c("state" = "State"))
pred_df$state <- c(state.abb, 'DC')[match(poll_2020_clean$state, c(state.name, 'District of Columbia'))]

pred_df <- pred_df %>% 
  left_join(demo %>% filter(year == 2018), by = c("state" = "state")) %>% 
  left_join(vep_df %>% filter(year == 2016), by = c("state" = "state")) %>% 
  dplyr::select(-year.x) %>% dplyr::select(-year.y) 

colnames(pred_df)[6] <- "rdi_q2"

pred_df <- pred_df %>% add_column(year = rep(2020, 102), total = NA, D = NA, R = NA, R_pv2p = NA,
                                  D_pv2p= NA, D_pv = NA, R_pv = NA, vote_margin = NA, winner = NA,
                                  inc_pv = NA, chl_pv = NA, incumbent_win = NA, inc_votes = NA, 
                                  chl_votes = NA) %>% 
  mutate(incumbent = case_when(party == "republican" ~ TRUE, 
                               party == "democrat" ~ FALSE)) %>%
  mutate(incumbent_party = incumbent, inc_party = incumbent, chl_party = !incumbent) 

reg_df$inc_party <- as.logical(reg_df$inc_party)
reg_df$chl_party <- as.logical(reg_df$chl_party)

vote_num <- c("rdi_q2", "Asian",
              "Black", "Hispanic", "Indigenous", "White", "Female", "Male", "age20",
              "age3045", "age4565", "age65")

full_votes <- rbind(reg_df, pred_df) %>% dplyr::select(-incumbent,-inc_party, -chl_party) %>%
  mutate(inc_party = incumbent_party, chl_party = !incumbent_party) %>% 
  group_by(year) %>%
  mutate_at(vote_num, scale) %>% 
  ungroup() %>%
  left_join(turnout_scaled %>% dplyr::select(state, year, last_vote_margin), by = c("state" = "state", "year" = "year"))

## build vote share model
inc_vote_model <- glm(cbind(inc_votes, total - inc_votes) ~ rdi_q2 + gdp + state + 
                        avg_poll + Black + Hispanic + Asian + White + Male + 
                        age20 + age3045 + age4565 + last_vote_margin + party, 
                        data = full_votes %>% filter(year < 2020, incumbent_party == TRUE), 
                      family = binomial) 

chl_vote_model <- glm(cbind(chl_votes, total - chl_votes) ~ rdi_q2 + gdp + state + 
                        avg_poll + Black + Hispanic + Asian + White + Male + 
                        age20 + age3045 + age4565 + last_vote_margin + party, 
                      data = full_votes %>% filter(year < 2020, incumbent_party == FALSE), 
                      family = binomial)

```


Estimating vote share happens roughly the same way estimating turnout, just with a different generalized linear model. Similar to many of the models I have used throughout this blog, I am using a two sided model based on party incumbency status[^4].  I still used a pooled model across all states and years since the 1992 election. 

For both the incumbent and the challenger models, I use a binomial regression, estimating the fraction of the total votes that each candidate will win. The regression uses demographic data, polling data, previous election results, economic data, along with party and state fixed effects. We can take a look at both the incumbent and challenger models. First, the incumbent model:

```{r, echo=FALSE,  warning = FALSE, message = FALSE, class.output="scroll-100" }
 summary(inc_vote_model)
```

We can also look at the challenger model:

```{r, echo=FALSE,  warning = FALSE, message = FALSE, class.output="scroll-100" }
 summary(chl_vote_model)
```

Again, for both models, the residual deviances are orders of magnitude greater than the null deviances. One thing to note is that the incumbent model seems to have a better fit in sample than the challenger model, with a null deviance of `r format(inc_vote_model$deviance, scientific = FALSE)` against `r format(chl_vote_model$deviance, scientific = FALSE)`. Unsurprisingly, the coefficient on the polling average is large and positive, relative to other coefficients. Interestingly, the economic data coefficients are positive in the incumbent model, but negative in the challenger model, which makes some intuitive sense. An incumbent party with a good economy is likely to have better success with voters[^5].

For out of sample fit, we can look at the fraction of states that the model predicts correctly using leave one out validation based on year. 

```{r echo = FALSE, return = FALSE, warning = FALSE, message = FALSE}
year_list <- seq(from = 1992, to = 2016, by = 4)

vote_share_check <- function(yr){
  temp_inc_df <- full_votes %>% filter(year != yr, year < 2020, incumbent_party == TRUE)
  temp_chl_df <- full_votes %>% filter(year != yr, year < 2020, incumbent_party == FALSE)
  
  if(yr != 2016){
    pred_inc_df <- full_votes %>% filter(year == yr, incumbent_party == TRUE)
    pred_chl_df <- full_votes %>% filter(year == yr, incumbent_party == FALSE)
  }else{
    pred_inc_df <- full_votes %>% filter(year == yr, incumbent_party == TRUE, state != "DC")
    pred_chl_df <- full_votes %>% filter(year == yr, incumbent_party == FALSE, state != "DC")  
  }
  
  temp_inc_vote_model <- glm(cbind(inc_votes, total - inc_votes) ~ rdi_q2 + gdp + state + 
                          avg_poll + Black + Hispanic + Asian + White + Male + 
                            age20 + age3045 + age4565 +last_vote_margin + party, 
                        data = temp_inc_df, family = binomial)
  
  temp_chl_vote_model <- glm(cbind(chl_votes, total - chl_votes) ~ rdi_q2 + gdp + state + 
                          avg_poll + Black + Hispanic + Asian + White + Male + 
                            age20 + age3045 + age4565 + last_vote_margin + party, 
                        data = temp_chl_df, 
                        family = binomial)
  
  temp_pred_inc <- predict(inc_vote_model, pred_inc_df)
  temp_pred_chl <- predict(chl_vote_model, pred_chl_df)

  states_correct <- (temp_pred_inc > temp_pred_chl) == (pred_inc_df$inc_pv > pred_inc_df$chl_pv)
  frac_right <- sum(states_correct)/length(temp_pred_chl)
  
  c(yr, frac_right)
  
}

outsamp_votes <- as.data.frame(lapply(year_list, vote_share_check))
```

```{r, echo = FALSE, return = FALSE, warning = FALSE, message = FALSE}
outsamp_votes <- t(outsamp_votes)
colnames(outsamp_votes) <- c("Year", "State Prediction Accuracy")
rownames(outsamp_votes) <- c()
kbl(outsamp_votes)  %>% kable_paper(full_width = F) %>% kable_styling(position = "center")
```

For the most part, the model is quite accurate. It misses a few states in some elections. As dicussed [last week](polarization.html), a very similar model misses predictions in swing states quite frequently. This model would have incorrectly called the 2016 election for Hillary Clinton. These incorrect predictions in part stem from reliance on polling data, without accounting for variances. Therefore, we need to introduce some uncertainty into the model. 

## National Polling Error
Much has been made of the possibility of national polling error. In 2016, a roughly 3 point national polling error meant that many were blindsided by Trump's narrow victory in a number of midwest states. To simulate that, I calculated the mean and variance for polling errors for both Democrats and Republicans in elections since 1992. Interestingly, both parties tend to outperform their polls, by 2.40 and 2.11 points for Democrats and Republicans respectively. 

## Simulating Elections
Returning to the basic framework of the model, we have:

$$\textrm{Votes}_{ic}\sim\textrm{Bin}(n_{i}, p_{ic})$$

Based on our predictions, we can write down $n_i$ and $p_{ic}$ with distributions as well:

$$n_i \sim \textrm{Pois}(\lambda_i)$$
$$p_{ic} \sim \textrm{Beta}(\alpha_{ic}, \beta_{ic})$$
In this case, $\lambda_i$ are the predicted mean turnout in each state. As previously, we can re-paramaterize $\alpha_{ic}$ and $\beta_{ic}$ and instead input the mean and variance. One thing to note is that the $\alpha$ and $\beta$ values are quite large, meaning that we can approximate the beta distributions with high accuracy using a normal distribution. This is key, as it gives an easy way to account for correlation between states: the [multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution). 

Instead of drawing 102 values one at a time for each vote share for each candidate, we can instead draw two sets of 51. The mean of each value will be the predicted vote share from the vote share models previously discussed. We also need a covariance matrix, which explains how vote shares in different states are related to one another. 

We can calculate a covariance model in two steps: first, building a similarity matrix that tells us how similar each state is to one another, using all of the non-categorical data from the vote share model. The key detail of this similarity matrix is that values are between -1 and 1. Values near 1 indicate high similarity (or correlation), and negative values indicate that states are very dissimilar. We can then scale this makeshift correlation matrix by standard deviations in the polls over the past several weeks to get a covariance matrix. The standard deviations in the polling averages that I use are quite small, suggesting a very stable race, and therefore a relatively small amount of uncertainty[^6]. 

The final step is the national polling error, which I draw from two independent [stable distributions](https://en.wikipedia.org/wiki/Stable_distribution), one for Biden and one for Trump. I intentionally picked stable distributions because they are "fat-tailed," meaning that events farther from the mean, holding variance fixed, happen more frequently than in a normal distribution. This choice is meant to introduce a potentially large amount of polling error, and therefore variance into the model. Something to note is that these terms are independent for each candidate: both Biden *and* Trump could have polling errors in their favor in this model[^7]. 

One thing to note is that in a sense, including both variance at the state and national level is double counting variance. Variance in a model at the national level is a direct function of variance at the state level. However, in the uncertain times brought on by COVID-19 and one candidate actively trying to discredit the election, I decided that increasing the variance seems reasonable. I'll denote this variable $s_{c}$. We can then re-write the basic structure of the model as:

$$\textrm{Votes}_{ic}\sim\textrm{Bin}(n_{i}, p_{ic}+s_c)$$

This leaves us with a complete model, from which we can draw simulated values in order:

1. Draw the vote shares from the multivariate normal distributions for each candidate. 

2. Draw the turnout values for each state from the poisson distributions. 

3. Draw the national polling error for each candidate and add to the vote shares. 

4. Draw from the binomial in each state to get the votes in each state. 

We can finally look at the results from the model over 10,000 draws. 

## Results
We can first look at the average electoral map based on the average vote shares in each state. 

```{r echo = FALSE, return = FALSE, warning = FALSE, message = FALSE}
results_df <- read.csv("../Data/prediction.csv")

ec_results <- aggregate(results_df$EC, results_df %>% dplyr::select(win, sim), FUN = sum)
biden_pv <- aggregate(results_df$biden_v, results_df %>% dplyr::select(sim), FUN = sum)
trump_pv <- aggregate(results_df$trump_v, results_df %>% dplyr::select(sim), FUN = sum)

avg_popvote_state <- results_df %>% mutate(trump_pv = trump_v/(trump_v+biden_v),
                                           biden_pv = biden_v/(trump_v+biden_v)) %>%
  group_by(state) %>%
  summarize(trump_pv = mean(trump_pv), biden_pv = mean(biden_pv)) %>%
  mutate(win = case_when(trump_pv > biden_pv ~ "Trump",
                         biden_pv > trump_pv ~ "Biden"))

avg_popvote_table <- avg_popvote_state %>% dplyr::select(state, trump_pv, biden_pv)
colnames(avg_popvote_table) <- c("State", "Trump Popular Vote", "Biden Popular Vote")


ec_wins <- ec_results %>% spread(win, x) %>% mutate(winner = case_when(Biden > Trump ~ "Biden", 
                                                                       Trump > Biden ~ "Trump", 
                                                                       Biden == Trump ~ "Tie"))


victors <- ec_wins %>% group_by(winner) %>% count()

```


```{r echo = FALSE, return = FALSE, warning = FALSE, message = FALSE}
pop_vote_plot <- avg_popvote_state %>% 
  ggplot(aes(state = state, fill = fct_relevel(win, "Trump", "Biden"))) +
  geom_statebins() +
  theme_statebins() +
  labs(title = "2020 Presidential Election Based on Average Popular Vote Share",
       subtitle = "From 10000 Simulations",
       fill = "") +
  scale_fill_manual(values=c("#619CFF", "#CCCCCC", "#F8766D"), breaks = c("Biden", "N/A", "Trump"))

pop_vote_plot
```

We can also look at the actual vote shares:

```{r,echo=FALSE,  warning = FALSE, results = 'asis', message = FALSE, class.output="scroll-100" }
kbl(avg_popvote_table)  %>% kable_paper(full_width = F) %>% kable_styling(position = "center") %>%
  scroll_box(width = "500px", height = "200px") %>% kable_styling(position = "center")
```

This model shows a blowout for Biden in a number of key states: he is up by more than ten points in Michigan, Florida, and Pennsylvannia. He also narrowly wins South Carolina, and nearly pulls out a victory in Texas. Overall, Biden wins `r mean(as.numeric(unlist(ec_results %>% filter(win == "Biden") %>% dplyr::select(x))))` electoral votes on average, with Trump picking up the remaining `r mean(as.numeric(unlist(ec_results %>% filter(win == "Trump") %>% dplyr::select(x))))`. Looking at the distribution of the electoral college outcomes fits exactly with this theme 

```{r echo = FALSE, return = FALSE, warning = FALSE, message = FALSE, results='hide'}
biden_ec_plot <- ggplot(ec_results %>% filter(win == "Biden"), aes(x = x, fill = win), alpha = 0.75 ) +
  geom_density(alpha =0.25, color = "dodgerblue2") +
  geom_histogram(bins = max(ec_results$x) - min(ec_results$x), aes(y = ..density..),position = 'identity') + 
  labs(x="Electoral College Results", y="Frequency", color = "Legend") + ggtitle("Predicted Electoral College Results") + 
  scale_fill_manual(values=c("dodgerblue2")) + 
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        panel.background = element_blank(), 
        axis.line = element_line(colour = "grey"),
        legend.key = element_rect(fill="transparent", colour=NA),
        legend.position = "none")+
  scale_color_manual(values=c("deepskyblue")) + 
  labs(title = "Biden Electoral College Votes from 10000 Simulations") + xlim(0,538)

trump_ec_plot <- ggplot(ec_results %>% filter(win == "Trump"), aes(x = x, fill = win), alpha = 0.75 ) +
  geom_density(alpha =0.25, color = "firebrick2") +
  geom_histogram(bins = max(ec_results$x) - min(ec_results$x), aes(y = ..density..),position = 'identity') + 
  labs(x="Electoral College Results", y="Frequency", color = "Legend") + ggtitle("Predicted Electoral College Results") + 
  scale_fill_manual(values=c("firebrick2")) + 
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        panel.background = element_blank(), 
        axis.line = element_line(colour = "grey"),
        legend.key = element_rect(fill="transparent", colour=NA),
        legend.position = "none")+
  scale_color_manual(values=c("firebrick")) + 
  labs(title = "Trump Electoral College Votes from 10000 Simulations") + xlim(0,538)

ec_plot <- gridExtra::grid.arrange(trump_ec_plot, biden_ec_plot)
ec_plot




```

We can see that there is very, very little overlap in the electoral college results. In fact, this model predicts Biden victory `r victors[1,2]` out of 10,000 simulations, while Trump wins the remaining `r victors[2,2]`. As extreme as this result is, it's not totally out of line with [The Economist's prediction](https://projects.economist.com/us-2020-forecast/president). We can also look at the plot of the popular vote shares, and we can again see very little overlap.  

```{r echo = FALSE, return = FALSE, warning = FALSE, message = FALSE, results='hide'}
pv_df <- biden_pv %>% left_join(trump_pv, by = c("sim" = "sim"))
colnames(pv_df) <- c("sim", "biden_votes", "trump_votes")
pv_df <- pv_df %>% mutate(biden_pv = biden_votes/(biden_votes+trump_votes), 
                          trump_pv = trump_votes/(biden_votes+trump_votes), 
                          win = case_when(biden_votes > trump_votes ~ "Biden",
                                          trump_votes > biden_votes ~ "Trump")) 

pop_vote_split <- sum(ec_wins$winner != pv_df$winner)

biden_pv_plot <- ggplot(pv_df %>% dplyr::select(biden_pv), aes(x = biden_pv), alpha = 0.75) +
  geom_histogram(bins = 100, 
                 aes(y = ..density..),
                 position = 'identity', 
                 color = "dodgerblue2",
                 fill = "deepskyblue") + 
  labs(x="Popular Vote Results", y="Frequency", color = "Legend") + ggtitle("Predicted Popular Vote Results") + 
  scale_fill_manual(values=c("dodgerblue2")) + 
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        panel.background = element_blank(), 
        axis.line = element_line(colour = "grey"),
        legend.key = element_rect(fill="transparent", colour=NA),
        legend.position = "none")+
  labs(title = "Biden Popular Vote Percent", subtitle = "From 10000 Simiulations") + xlim(0,1)

trump_pv_plot <- ggplot(pv_df %>% dplyr::select(trump_pv), aes(x = trump_pv), alpha = 0.75) +
  geom_histogram(bins = 100, 
                 aes(y = ..density..),
                 position = 'identity', 
                 color = "firebrick",
                 fill = "firebrick2") + 
  labs(x="Popular Vote Results", y="Frequency", color = "Legend") + ggtitle("Predicted Popular Vote Results") + 
  scale_fill_manual(values=c("dodgerblue2")) + 
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        panel.background = element_blank(), 
        axis.line = element_line(colour = "grey"),
        legend.key = element_rect(fill="transparent", colour=NA),
        legend.position = "none")+
  labs(title = "Trump Popular Vote Percent", subtitle = "From 10000 Simiulations") + xlim(0,1)

pv_plot <- gridExtra::grid.arrange(trump_pv_plot, biden_pv_plot, ncol = 2)
pv_plot
```

This plot shows Biden with a commanding lead in the national popular vote, winning above 50% of the vote in essentially every contest. There is an electoral college and popular vote split in `r pop_vote_split` of the 10,000 contests, meaning that Trump only wins when he happens to win the popular vote. Texas and South Carolina coming down to razor thin margins fits with the heavily Democrat leaning national enviornment.

## Initial Model Assessment
While this model is much more sophisticated than anything else that I have built this past semester, I think it is still flawed. While the state to state correlation and national polling error terms are the most interesting model I have built, I did not spend much time fine tuning them. State to state correlation is certainly something I wish I spent more time on, as this is a feature that is notoriously hard to get right[^8]. I also wish I had the chance to run the full model on previous elections, conducting yet another round of out of sample validation. It would be interesting to see how this model performs with data that do not suggest a blowout election. One other note is that this model seems to understate the value of the electoral college - other models that I have seen do have the possibility of a electoral college and popular vote victor split, which did not occur here.

All in all, this model gives Biden an extremely high chance of victory, which does make sense given the national environment This model is quite certain of the victory, but everyone should remember that events with small probabilities do happen occasionally, no matter how small the probability is. For example, the rouhgly 0.1% chance that my model gives Trump is roughly the chance of three randomly selected people all being left handed. Also, frankly, I think my model is way too certain. I think further experimentation with variable selection might introduce more uncertainty, or changing how polling variance is thought of. 

What this model suggests is that for Donald Trump to win re-election, he needs a massive polling error in his favor, or he needs courts to allow him to conduct extra-legal shenanigans that throw out votes. One final thing to remember is this: all models are wrong, but some models are useful. Undoubtedly, this model will be wrong in the exact votes, but it can still tell us that Biden has a very, very good chance of victory. 

## Appendix: Data
Data for this model came from a number of places. In some situations, I also scaled some of the data so that different measures had approximately the same magnitude. 

1. Polling data. For historical data, this was an average of state level polls over the final three weeks of the election. This is a very crude form of poll aggregation, averaging over time. The choice of three weeks comes from striking a balance between using polls that are recent enough, but also making sure not to limit the amount of data I have. Averaging over time is a strategy used by both [FiveThirtyEight](https://fivethirtyeight.com/features/how-fivethirtyeights-2020-presidential-forecast-works-and-whats-different-because-of-covid-19/) and [The Economist](https://projects.economist.com/us-2020-forecast/president/how-this-works). For 2020 data, I used the smoothed polling average from [FiveThirtyEight](https://projects.fivethirtyeight.com/2020-election-forecast/?cid=rrpromo), again taking the average over the final three weeks. One added benefit is that pollsters tend to herd in the final week or so before the election - meaning that no one wants to be an outlier, so incorporating data from farther back helps negate this effect.

2. Economic data. This comes both from sources provided in class, and from the Bureau of Economic Analysis. I used two pieces of data: second quarter GDP at the national level, and real disposable income at the state level. Real disposable income data comes from [“Quarterly Personal Income By State.”](https://apps.bea.gov/itable/iTable.cfm?ReqID=70&step=1). GDP data comes from class. The reasoning for including a mix of both economic indicators and levels is to cover the widest range of possible impact on voting behavior, and to make sure that a single indicator from 2020 does not completely skew the results. Given that we are in a recession, something like GDP will show a massive loss in the second quarter, while real disposable income will show a huge gain (due to the CARES Act). As mentioned in the footnotes, there is significant evidence that the economy does impact how people vote.

3. Demographic data. I use a range of demographic data at the state level. In both the turnout and vote share estimation regressions, I used the same set of data: fraction of the population that is white, black, hispanic, asian, age 20 to 30, age 30 to 45, and age 45 to 65. There are other race or age categories that I could include, but I wanted to avoid collinearity - you can figure out roughly how many people there are over the age of 65 by adding up all the other categories. This data came from the dataset provided in class. One problem in this case is that the dataset I was using only had demographic data up to 2018, which is what I substituted for 2020. A common refrain in predictive models is "garbage in, garbage out," meaning that this could cause problems for predictions. There were two related reasons to include demographic information: first, it acts as a way to have correlation between states, and also because certain demographic groups tend to vote in particular ways. For example, African American women [vote overwhelmingly](https://www.cnn.com/2020/10/21/politics/black-women-voters-michigan/index.html) for Democrats. 

4. Indicators. These are dummy variables, primarily for party and state. Given the use in a pooled model, they are meant to capture time invariant fixed effects. Because I am primarily working with a two sided model based on incumbency, adding in a control for the party seems like a reasonable thing to do to capture some amount of the effect of political polarization. State fixed effects are meant to account for the particular impact of each state, assuming that it is constant over time, which could account for things like cost of voting. 

5. Lagged data. This includes lagged vote share data, along with lagged turnout data when estimating turnout. The reason for including this information is simple: the best predictor of the future is the past. 

One important thing I did early on in the process was scale a large amount of my data to have a mean of zero and standard deviation of one, within each year. I scaled previous turnout, vote margin, demographic data, and real disposable income data based on each year[^9]. This was primarily to ensure that past turnout data did not completely overwhelm everything else, as it was on much larger  order of magnitude. I intentionally did not scale the polling data, because I needed the variances in raw form to incorporate into the model at other points. 



[^1]: There are some assumptions that should hold for a variable to be distributed poisson, and I believe that most of them should hold. The first is that individual votes being cast happens independently from one another (for example, me casting a vote does not make you in particular any more or less likely to cast a vote). Second, the mean should be roughly equal to the variance, which in this case it is not. The mean of total votes cast is roughly $2\times10^7$, while the variance is roughly $5.6\times10^{12}$, meaning that we have overdispersion. For simplicity in the simulation process, I decided to stick with using a poisson regression, even if it does mean that the variance in turnout will be lower. 

[^2]: In general, states that are more competitive see surges in turnout. See [Li et Al. (2018)](https://www-liebertpub-com.ezp-prod1.hul.harvard.edu/doi/pdf/10.1089%2Felj.2017.0478) and [Bursztyn et Al. (2017)](https://www.nber.org/system/files/working_papers/w23490/w23490.pdf).

[^3]: A model that is just the mean outcome variable.  

[^4]: As discussed previously, [incumbency matters](incumbency.html) a great deal. To account for this, I split the data and generate two seperate models - one for candidates from the incumbent party, and a second for the challenger party. As explored last week, this performs roughly as well as party based two sided models. One thing to note is that it is by party status, rather than for the candidates. Some models, like Abramowitz's [time for change model](https://pollyvote.com/en/components/models/retrospective/fundamentals-plus-models/time-for-change-model/), which has been quite successful only use actual incumbency status. 

[^5]: See [Achen and Bartels (2017)](https://muse-jhu-edu.ezp-prod1.hul.harvard.edu/book/64646) for further evidence of this claim.

[^6]: One thing to note about this is that another way to measure uncertainty would have been to use the standard deviations from each state in individual polls, rather than in the average. I decided against this because individual polls can be quite noisy, and poll aggregators like FiveThirtyEight do a good job of putting together a trend line that removes a lot of the noise. 

[^7]: This is less of a modelling choice, and more about convenience as generating draws from correlated stable distributions. In fact, the covariance between errors is negative, as expected, meaning that this is a clear limitation on the model. However, it does fit with the idea that both Democrats and Republicans consistently beat their polls. 

[^8]: See G. Elliot Morris, Nate Cohn, and Nate Silver's twitter feeds for examples, along with this [blog post](https://statmodeling.stat.columbia.edu/2020/08/31/problem-of-the-between-state-correlations-in-the-fivethirtyeight-election-forecast/) from Andrew Gelman.

[^9]: So that within each year, the mean value is 0 and the standard deviation is 1. I did not scale the GDP as it would all be set to mean 0, as it was the same for all states. 